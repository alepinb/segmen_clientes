{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos como en tu código original\n",
    "# Asumiendo que ya tienes X_train_scaled, X_val_scaled, y_train_encoded, y_val_encoded preparados\n",
    "\n",
    "def create_model(trial):\n",
    "    # Optimización de hiperparámetros\n",
    "    # Definir el número de neuronas por capa\n",
    "    num_neurons_1 = trial.suggest_int('num_neurons_1', 32, 128)\n",
    "    num_neurons_2 = trial.suggest_int('num_neurons_2', 16, 64)\n",
    "    \n",
    "    # Regularización L2\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-2)\n",
    "    \n",
    "    # Tasa de aprendizaje\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    # Crear el modelo\n",
    "    model = Sequential([\n",
    "        Dense(num_neurons_1, activation='elu', kernel_regularizer=l2(l2_reg), input_dim=X_train_scaled.shape[1]),\n",
    "        Dense(num_neurons_2, activation='elu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dense(y_train_encoded.shape[1], activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Crear el modelo con los hiperparámetros sugeridos\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Definir el EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train_encoded,\n",
    "        epochs=200,\n",
    "        batch_size=8,\n",
    "        validation_data=(X_val_scaled, y_val_encoded),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Obtener la exactitud en el conjunto de validación\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    return val_accuracy  # Maximizar la exactitud en validación\n",
    "\n",
    "# Crear el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Ejecutar la optimización\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Evaluar el modelo con los mejores hiperparámetros en el conjunto de test\n",
    "best_model = create_model(study.best_trial)\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train_encoded,\n",
    "    epochs=200,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val_scaled, y_val_encoded),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
    "print(f\"Exactitud en test con los mejores hiperparámetros: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
